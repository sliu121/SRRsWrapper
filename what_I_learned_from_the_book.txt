What is a metasearch engine? There are too many explaination you can find on Internet. Here is what I think. Metasearch is not a 'search engine'! It's a 'thief'! It 'steals' the result
from other search engines like Google, Bing, etc. But metasearch will analyze all the result it got, and give user the best result. So, metasearch is a search engine of search engine,
and best part of it is drawing irrelevant and redundancy results by using some algorithm. From the Ch4 intro part, we could know, if we want to establish a connection between 
a metasearch engine and a search engine, there are 2 things we should do: connector and result extractor. 

Before talking about these two, let's talk about HTTP form tag first. In an HTTP form of search engine, there contains an input control of text type( creat an input field), an action 
attribute( specifies the name and location of search engine server to which the user query will be sent when the form is submitted), a submit input control ( define the name of 
the submit button), and a method attribute (specifies the HTTP method for sending the query to the search engine server). In method, there are 2 types we need to understand, 
get and post. Get has a limited size of user input and characters user can use, and also permits only the retrieval of information from a remote server, and foremost it is not secure 
for transmitting important data over the Internet. In contrast, there is no limit size or character on post, and post may request storing and updating data in addition to retrieving 
information. It is not easy to understand right? Let's talk it more noob way, my way. Get likes 'SELECT' in SQL, and post likes 'UPDATE', one is for getting information from server and
the other one is for sending information to update/ create resource for server. Not only these reasons decide we use get more frequently, also, search engines users don't need 
high-level secure and their inputs are short; get request can be bookmarked, for those same search input, server could give answer more quickly. 

However, GET is not always the best choice. When we use a search engine, what we do? first input what we want to search, then click submit, finally get what we want. During this 
operation, after submitting what we want to search, there will generate a new URL first and it includes "action +?+ input w/ other default info ". Action is the server, input is characters 
you inputted, and between them, there is a ?. Or we could say that first ? in URL separates action and GET request message. Hold on, did you find what I just said above this line. 
Yes, GET request message =(likely) input w/ other default info, And in this message all the characters you inputted will be shown. Now you know there are some problem, how 
about my username and password? So, thats why we need POST. In POST request message, our inputs  length and type will be shown instead of content. 

Now, since we have learned how HTTP works, then let's talk about connector. Why we need connector? Because metasearch engine usually cannot utilize a browser to transmit 
the HTTP request message and recieve the HTTP response message. Now how to generate a connector? As we already talked above, a search engine need send a request 
message to the server, so what is the format or info we need for this request? Here it is: name and location of the search engine server, the HTTP request method, The name of each
input control in the search form (except the submit type) and its value if exists. Since we are CS students or computer workers, we need to 'make' an automatic search connector,
which could find and get the info from pages automatically. First step, how to find? we know that all these info are in <form>..</form>, if there only contains one input control 
and it is for search, that is really easy. However, most time there are multiple input controls in one web page, and we have to recognize the correct one. There are two solutions we
could use. First one, checking a) if there is a text input field in <form> and b) at least one of these keywords such as " search,” “query” or “find” appears, either in the form tag or 
in the text immediately preceding or following the form tag. The other one, using a decision tree(details are not including here). However, in reality, we could not guess which 
search engine a user looking for. So, here is our team job, find all the possible search engines for users. 
 
 After finding these info we need, next step is extracting the search form. In most cases, it is very easy since we could find name and location of search engine server in action 
 attribute, HTTP request method from method attribute, and the name of each input control with its value from the name and value attributes of the input tag. However, in some
 cases, we cannot find the name and location of search engine server easily since there is a relative URL or no-URL at all in action, then we need to construct a URL for HTTP 
 request message. For relative URL, there are two types about it. One is without '/' and the other one is. For the former one, we need to combine it with relative or current URL
 we have and then construct a new one. For the latter case, all we need to do is combine with domain name of the current URL. 

 Ok, now we have already know how to construct a connection with search engines, we should look how to extract result from those search engine. As we all know, after 
 receiving query from user, server will give a lot of response pages and we call those pages as Search result records (SRRs). However, in those response pages, there are some 
 unnecessary items such as advertisements or something user not looking for, then metasearch engine's job is extracting the correct SRRs and sorting them for users. We call 
 this extracion as extraction wrapper. In theses days, we have already had 2 main method. One is, treating every page as a new page and analyzing them every time, even 
 some of them may already be analyzed before. For every students, we should know that there are many disadvantages in this method, and for CS students, we know how 
 to call this method, brute force, which equals to high time cost. As a result, we would not talk this one here. We will talk the other one, generating the extraction rules from 
 sample response pages and apply the rules to perform extraction for new response pages returned from the same search engine. This method just like machine learning type,
 analyze samples and give the prediction for a new case. And we call this extraction method as wrapper-based approach. And according how it works, we have three different 
 types of extraction. Manual, which asks a high-level skills developer to analyze from HTML source files of samples and identify the patterns or rules for extracting SRRs; 
 semiautomatic, which need human manually mark correct SRRs and machine will generate a rule according to human work. Those 2 methods are not good enough. Semiautomatic
is  not good, cause we are CS students, we create computer because we dont want to work, also manual really need a strong- experienced developer for identification. So, 
as you already guess, we gonna talk about the final method, automatic approach which we mentioned above several times. But before to this real topic, we should learn some
old-time machine, Semiautomatic approach.

    1. Lixto. This method is very easy to understand. This wrapper generation tool extracts desired information from HTML and saves them in XML files. How it works? First, the users
open the web page in a Lixto browser which makes the page to be markable. Then, users mark the desired information, and tool will generate some patterns consist of many 
filters as users mark. Finally, tools will analyze those other pages according to filters given by users. If a piece of information has been recognized as a desired one, then it should 
satisfy all the conditions of at least one filter of pattern. So, it kind of likes we give an sample to machine, and tell it which part is right, and let it give us the correct answer for 
other same questions. So, for this method, we may get 3 types of result. First, every desired information is matched and no undesired information is matched, this is perfect result 
we want, this indicates that our patterns, filters, and conditions work good. Second, some desired information are not matched, this means we need modify some conditions to 
enlarge the correct range. Third, some undesired information are matched, this means we need restrict conditions of filters. Now, we could learn that this method is an iterative
method, we modify these conditions of filters again and again until we get the final result we want. 

    2. WIEN. WIEN, wrapper induction environment, supports a family of wrapper classes mostly designed for extracting data records in tabular format. What is tabular format? 
Remember database, yeah, knid of like that, every data has a lots of attributes, so SRRs is pretty proper for this situation, look at result pages Google showing us every time, 
title, sinppet and URL. The one of classes is HLRT, which uses delimiters to seperate each values in one record. Suppose there are 4 values or 
attributes in a record, and we need 8 delimiters to seperate these values or attributes, 2 for one attribute, after that, we need to seperate records, so 2 for one record, then we 
have the conclusion, for one record with K values, we need 2K +2 delimiters. Also, we should identify some irrelevant information by using 2 delimiters to divide it with other 
information we need. This is HOCLRT. 

    3. Thresher. I still have some confusion in this part. To understand this part well, I should read more tree edit distance and dynamic programming.

Now let us talk about automatic wrapper generation. This wrapper is an unsurpervised techniques since there is no human works, and also can be applied to Web pages with 
multiple objects of the same type because the multiple similarly structured objects make it possible to discover the repeating pattern among these objects. There are three most
popular methods we are using these days. First, recognizing visual information. As we all know, for every part in a web page, it is always separated and emphsized with different 
colors. Recently, we have such technique could help us recognize those visual information, then we could use some applications to utilize those visual information on rendered 
Web pages to perform data extraction. ViDRE is the most famous mothed that only use visual information in wrapper generation. Second, tag information. For HTML documents, 
every content are wrappered in HTML tags. Every documents can be either represented as a string of tags and text tokens or as a DOM/tag trees. Both of them are used in early
automatic wrapper generations, such as RoadRunner, Omini, DeLA, and EXALG. However, this method is not that good. Foe example, there is not such a convention on tag usage
which means there is not too much reliable information to tell which part is exactly found in which part, also with Web techniques blooming, pages have many different formats
and styles, which cuases more effort. The last reason why tag information is not good enough is robust programming languages. HTML is not the only language we are using today, 
like XML, XHTML are introduced today. And every time HTML is updated, the wrapper generation need be updated too, which makes it not look like 'intelligent'.  So some people
 put tag information and visual information together, such as ViNTs, ViPER, DEPTA. The last one is, domain information. I can't explain it well, after reading this part, maybe next 
 example could help me understand better. Now it's time for some examples.  

    1. Omini, one of the earliest automatic wrapper generation systems for extracting SRRs from Web pages and only uses tag tag information. 





